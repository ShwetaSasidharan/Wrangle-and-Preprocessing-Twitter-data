{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Date: 12/09/2020\n",
    "\n",
    "##### Version: 1.0\n",
    "\n",
    "#### Environment: Python 3.7.4 and Jupyter notebook\n",
    "\n",
    "### Libraries used:\n",
    "\n",
    "* os (for loading the system path and files, included in Anaconda Python 3.7)\n",
    "* re (for regular expression, included in Anaconda Python 3.7)\n",
    "* langid (for language classification, included in Anaconda Python 3.7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to import libraries \n",
    "from langid import classify\n",
    "import os \n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2. Parse the text Files under /31224075/\n",
    "\n",
    "\n",
    "* Using the os directory to list all the files within the directory\n",
    "\n",
    "* Initializing the directory with the files stored and then creating a file list to sort the files in the order of filenames. \n",
    "\n",
    "* The final fileslist is used to read the files from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.91 ms, sys: 0 ns, total: 1.91 ms\n",
      "Wall time: 2.47 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#Initializing the data directoty path\n",
    "dir = \"31224075/\" \n",
    "\n",
    "'''\n",
    "The os.listdir changed the file order in which the file was read, so I had to incorporate the file sort logic to match my\n",
    "date logic. \n",
    "'''\n",
    "\n",
    "#Ordering the files in the directory according to the file name i.e date order\n",
    "filelist = []\n",
    "for file in os.listdir( dir ):\n",
    "  if file.endswith( \".txt\" ):\n",
    "      filelist.append(file)\n",
    "filelist.sort()  # sort file names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initializing the variables used by the pre processing steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 1e+03 ns, total: 6 µs\n",
      "Wall time: 9.78 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Initializing empty dictionaries to be used\n",
    "tweet_dict = {}\n",
    "date_dict = {}\n",
    "final_text = ()\n",
    "count_tweets = 0\n",
    "count_english_tweets = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 . The steps and approach followed for the below task: \n",
    "\n",
    "1. Now with the files in hand, it was neccessary to find the best way to store the data for the 2418 files. My initial approach was to store two dictionaries with date and text & another with ID and text. However, to optimize the runtime and make the code more efficient I made changes to store the data only in one dictionary. \n",
    "\n",
    "2. Another approach where in I improved the runtime complexity is by extracting the date from the file name and not the created_at field which added to the run time initially.\n",
    "\n",
    "> FileName : 2020-03-22_235 --> The first 10 substring is the date input and this format is seen to be standard in the list of files. </font>\n",
    "\n",
    "3. Withheld pattern was an exception found when I ran the file folder for submission. Added an extra regex to handle this situation. \n",
    "\n",
    "4. After reading the file, and the file lines, start by wrangling the data by matching the {} for the data contents in the file.\n",
    "\n",
    "The text data file given is in the format of : \n",
    "\n",
    "> {\"data\":[{\"text\":\"DADO QUE A GRANDE MÍDIA E O PRÓPRIO  MISTÉRIO DA SAÚDE NÃO MOSTRA A POPULAÇÃO. https://t.co/XHsulXGJ2p\", \"created_at\":\"2020-06-07T15:08:30.000Z\", \"id\":\"1269647427436515332\"},{\"text\":\"@guardian Weekend #Fatality Hangover - 77 #Covid19 Died is a Lie, True Figs come out from Tues till nxt W/e #BorisJohnson #coronavirus #Crimesagainstsociety #Blamegame #buckstopsatthrTOP https://t.co/F7aQs2e8Ld\", \"created_at\":\"2020-06-07T15:08:30.000Z\", \"id\":\"1269647427562323970\"},...,]} \n",
    "\n",
    "So the first pattern in the regex looks for everything inside the {} in the file lines.  \n",
    "\n",
    "5. If the pattern matches, look for the text pattern in the matched groups. Retrive the final_text with the matched groups.\n",
    "\n",
    "> Text Pattern - \"text\":\\s*\"(.*?)(?<!\\\\)\"\n",
    "    * \"text\": matches the characters \"text\": literally (case sensitive)\n",
    "    \\s* matches any whitespace character (equal to [\\r\\n\\t\\f\\v ])\n",
    "    * Quantifier — Matches between zero and unlimited times, as many times as possible, giving back as needed (greedy)\n",
    "    \" matches the character \" literally (case sensitive)\n",
    "    1st Capturing Group (.*?)\n",
    "    .*? matches any character (except for line terminators)\n",
    "    *? Quantifier — Matches between zero and unlimited times, as few times as possible, expanding as needed (lazy)\n",
    "    Negative Lookbehind (?<!\\\\)\n",
    "    Assert that the Regex below does not match\n",
    "    \\\\ matches the character \\ literally (case sensitive)\n",
    "    \" matches the character \" literally (case sensitive)\n",
    "    \n",
    "    Text_pattern - Captures everything in between double quotes which doesn't have a preceding backslash \n",
    "\n",
    "\n",
    "For ex: \n",
    "\n",
    ">  \"text\":\"@guardian Weekend #Fatality Hangover - 77 #Covid19 Died is a Lie, True Figs come out from Tues till nxt W/e #BorisJohnson #coronavirus #Crimesagainstsociety #Blamegame #buckstopsatthrTOP https://t.co/F7aQs2e8Ld\"\n",
    "\n",
    "```python\n",
    ">  final_text= text.group(1)  \n",
    "  \n",
    ">  match.group(1) will then return :   \n",
    "```  \n",
    "  \n",
    "  > @guardian Weekend #Fatality Hangover - 77 #Covid19 Died is a Lie, True Figs come out from Tues till nxt W/e #BorisJohnson #coronavirus #Crimesagainstsociety #Blamegame #buckstopsatthrTOP https://t.co/F7aQs2e8Ld\n",
    "  \n",
    "6. With the final text, check if the final text is classified as English using langid's classify method. And proceed to the further pre processing steps only if it passes this condition. \n",
    "\n",
    "> langid.py is a Standalone Language Identification (LangID) tool.\n",
    "\n",
    "7. Similarly looking for the ID tags within the match groups for the {} pattern. \n",
    "\n",
    "> ID Pattern: - '\"id\":\\s*\"(.*?)\"'\n",
    "\n",
    "8. Now that I have parsed the string from the text, it is also required to look for emoji characters from the text. \n",
    "\n",
    "9. The emoji characters are expressed in surrogate pairs, the term \"surrogate pair\" refers to a means of encoding Unicode characters with high code-points in the UTF-16 encoding scheme. \n",
    "\n",
    "10. The text after you read the file are not \"surrogate pairs\" - instead, they are backslash-encoded codepoints for surrogate pairs, encoded as text. Hence the need to encode the text as string to ascii with backslahreplace. \n",
    "\n",
    "> Backslashreplace is an error handler which replaces any unencodable chars into backslash replacements.\n",
    "\n",
    "11. Now with the special \"unicode escape\" code, decode it back. This will preserve the other characters on the string and then decode it back using the same codec. At that point, both surrogate characters will be text as two characters. Then encode and decode it together, to get the actual surrogate codepoints that would be valid utf-16 to be decoded. The final decode from utf-16 will finally understand the surrogate pair as a single character. \n",
    "\n",
    "12. For the special characters in the text like Ampersand , Less-than, Greater-than, Quotes, Apostrophe needs to be escaped as &amp; &lt; &gt; &quot; and &apos; .\n",
    "\n",
    "13. Replace the special characters with the escaped XML form characters.\n",
    "\n",
    "14. Finally I'm storing the final text tuple in the dictionary with the date as values and the key as the ID.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 55.3 s, sys: 14 s, total: 1min 9s\n",
      "Wall time: 36.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Reading the files in the sorted filelist\n",
    "for file in filelist:\n",
    "    #Substring to get only the date part from the file name\n",
    "    date = file[:10]\n",
    "    with open( os.path.join( dir, file ) ,\"r\", encoding='UTF-8') as fd: \n",
    "        #Reading the file lines\n",
    "        file_lines = fd.readline() #.strip()\n",
    "        \n",
    "        #Parsing the withheld pattern found in one of the files\n",
    "        withheld_pattern = '\"withheld\":\\s*{(.*?)}'\n",
    "        if re.search(withheld_pattern, file_lines):\n",
    "            #Replacing the withheld pattern to null\n",
    "            file_lines = re.sub(withheld_pattern, \"\", file_lines)\n",
    "        \n",
    "        #Searching for pattern between the {} brackets\n",
    "        pattern = '\\s*{(.*?)}'\n",
    "        \n",
    "        #For every match found with the pattern in the file line\n",
    "        for match in re.finditer(pattern, file_lines):\n",
    "            count_tweets +=1\n",
    "            # Find the following text pattern from the matched record\n",
    "            text_pattern = '\"text\":\\s*\"(.*?)(?<!\\\\\\)\"'\n",
    "            # For every matched text pattern check if the match group returns a value\n",
    "            if re.search(text_pattern, match.group(0)) is None:\n",
    "                continue\n",
    "            # If a value is returned, store the text as text \n",
    "            text = re.search(text_pattern, match.group(1))\n",
    "            #print(\"textttt >>>>>>\" +text)\n",
    "            # Store the results to the final text \n",
    "            final_text= text.group(1)\n",
    "            #print(\"finallllllllll>>>\" +final_text)\n",
    "            \n",
    "            # Checking for English tweets using langid classify \n",
    "            x = classify(final_text)\n",
    "            \n",
    "            # If the text is English it will store the values in the dict with the ID as the key along with the date (from the file_name)\n",
    "            if x[0] == 'en':\n",
    "                count_english_tweets +=1\n",
    "                # Similarly check for the ID tag from the below regex:\n",
    "                id_pattern = '\"id\":\\s*\"(.*?)\"'\n",
    "                ID = re.search(id_pattern, match.group(0))\n",
    "            \n",
    "                # Encoding the final text with ascii and backslashreplace\n",
    "                # Which returns a bytes representation of the Unicode string, encoded in the requested encoding. \n",
    "                # Where in Backslashreplace inserts a \\uNNNN escape sequence)\n",
    "\n",
    "                final_text = final_text.encode('ascii', 'backslashreplace')\n",
    "\n",
    "                # Decoding the emoji characters using surrogatepass\n",
    "                final_text = final_text.decode('unicode-escape').encode('utf-16', 'surrogatepass').decode('utf-16')\n",
    "\n",
    "                # Replacing the XML special characters in their escaped form in the final text\n",
    "                xml_escaped =[\n",
    "                    (\"&amp;\", \"&(?!#\\d{4};|apos;|quot;)\"),\n",
    "                    (\"&quot;\", '\"'),\n",
    "                    (\"&apos;\", \"'\")\n",
    "                    #(\"&gt;\", \">\"),\n",
    "                    #(\"&lt;\", \"<\")\n",
    "                    ]\n",
    "\n",
    "                # Replacing the special characters found with their escaped forms :\n",
    "                for xml_char , og in xml_escaped:\n",
    "                    final_text = re.sub(og, xml_char, final_text)\n",
    "\n",
    "                tweet_dict[ID.group(1)] = [final_text, date]\n",
    "            else:\n",
    "                continue\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Creating the output XML File\n",
    "\n",
    "1. According to the dates in the dictionary, write the tweets in the the tweets tags with the ID and text. \n",
    "2. When a new date is read, end the date and start the the new date sequence and tweets for it.\n",
    "3. And end the XML file with the closing tags.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32.4 ms, sys: 86 µs, total: 32.5 ms\n",
      "Wall time: 31.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Writing the data to the XML file\n",
    "\n",
    "fw = open('31224075_test.xml', 'w', encoding='UTF-8')\n",
    "fw.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n')\n",
    "fw.write(\"<data>\\n\")\n",
    "\n",
    "# Date Logic :\n",
    "# Let the initial date be stored as default\n",
    "temp_date = 'default'\n",
    "\n",
    "# Passing the tweet dictionary with text and date\n",
    "for k, v in tweet_dict.items():\n",
    "    date = tweet_dict[k][1]\n",
    "    #dt = dt[:10]\n",
    "    #Checking if the temp_date as well as the current date is not equal to the date\n",
    "    if(temp_date != date):\n",
    "        if(temp_date != 'default'):\n",
    "            fw.write(\"</tweets>\\n\") #Marks the end of tweets tag if date is different\n",
    "        \n",
    "        #Sets the current date as date and starts the tweets tag with date\n",
    "        temp_date = date\n",
    "        tweet_date = '<tweets date=\"'+temp_date+'\">\\n'\n",
    "        fw.write(tweet_date)\n",
    "    \n",
    "    #Write the tweet with the ID from the key and text as the value\n",
    "    tweets = '<tweet id=\"'+k+'\">'+v[0]+'</tweet>\\n'\n",
    "    fw.write(tweets)\n",
    "\n",
    "# Final file write in the end closing tags      \n",
    "fw.write(\"</tweets>\\n\")\n",
    "fw.write(\"</data>\")\n",
    "fw.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The total number of files for the XML folder: 176 files\n",
      " The average file processing time : 17 mins \n",
      " The total no. of tweets in the file : 17600\n",
      " The total no. of English tweets processed is : 8725\n"
     ]
    }
   ],
   "source": [
    "#Give a short summary of your work done above, such as your findings.\n",
    "\n",
    "print(\" The total number of files for the XML folder: \" + str(len(filelist)) + \" files\" ) \n",
    "print(\" The average file processing time : 17 mins \")  \n",
    "print(\" The total no. of tweets in the file : \" + str(count_tweets)) \n",
    "print(\" The total no. of English tweets processed is : \" + str(count_english_tweets))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Learnings: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While converting a file from one input format to another, there is always a need to look out for the common occurences of a particular format in both. Based on that, leverage the use of regex to match the data needed in a required format. However, using regex may not always be a better option, as there are many libraries and API's doing the same task in a simpler way.\n",
    "But one can always find an answer using a regex pattern. \n",
    "\n",
    "Text language classification can be done by a powerful library called as langid, which recognizes over 95 different languages. Based on different use cases, langid can be modified to be used as per needs. \n",
    "\n",
    "Encoding is a major factor while reading and writing a file. One should take care about the special characters in the file and the escape sequences for a particular format. Here in this use case, emoji's had to be encoded in utf-16 and various special characters needed to be escaped in the final format. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Reference \n",
    "\n",
    "1. https://docs.python.org/2/library/re.html\n",
    "2. https://stackoverflow.com/questions/58949094/converting-surrogate-pairs-to-emoji-python3\n",
    "3. https://www.nltk.org/howto/collocations.html\n",
    "4. https://www.advancedinstaller.com/user-guide/xml-escaped-chars.html\n",
    "5. https://www.tidytextmining.com/ngrams.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
