{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Date: 12/09/2020\n",
    "\n",
    "##### Version: 1.0\n",
    "\n",
    "#### Environment: Python 3.7.4 and Jupyter notebook\n",
    "\n",
    "### Libraries used:\n",
    "\n",
    "* os (for loading the system path and files, included in Anaconda Python 3.7)\n",
    "* re (for regular expression, included in Anaconda Python 3.7)\n",
    "* langid (for language classification, included in Anaconda Python 3.7)\n",
    "* pandas (for dataframe manipulation, included in Anaconda Python 3.7)\n",
    "* nltk (for natural language toolkit, included in Anaconda Python 3.7)\n",
    "* sklearn (for machine learning, included in Anaconda Python 3.7)\n",
    "* itertools (for list manipulation , included in Anaconda Python 3.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing the libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langid import classify\n",
    "import re\n",
    "from itertools import chain\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.collocations import BigramAssocMeasures as bigram_measures\n",
    "from nltk.collocations import BigramCollocationFinder as bigram_finder\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import MWETokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initializing the various lists, dictionaries and the file path for the input file used in the program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing the tokenstoken_list =[]\n",
    "data_dict = {}\n",
    "data_list = []\n",
    "word_list = []\n",
    "clean_dict = {}\n",
    "token_dict = {}\n",
    "bigram_dict = {}\n",
    "unigram_dict = {}\n",
    "index_dict ={}\n",
    "without_R_tokens = {}\n",
    "stemmer = PorterStemmer()\n",
    "file_path = \"31224075.xlsx\"\n",
    "token_list =[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Parsing and reading the Input Excel File | Reading the stopwords file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Creates a Excelfile object\n",
    "file = pd.ExcelFile(file_path)\n",
    "\n",
    "# Reading the stopwords file for context independent stopwords removal\n",
    "with open('stopwords_en.txt','r') as stopwords:\n",
    "        stopwords_list = [line.strip() for line in stopwords]\n",
    "stopwords.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Reading the Excel File and Pre-processing the data\n",
    "\n",
    "### The mains pre-processing tasks done in this include: \n",
    "#### 1. Tokenization\n",
    "#### 2. Remove tokens less than length 3.\n",
    "#### 3. Remove stopwords (Context Independent stopwords)\n",
    "\n",
    "\n",
    "* Parse each sheet, to find the field with text, we need to drop all the NA's from the file. This is done using the dropna when all the valus in the row and columns are NA's. \n",
    "* Pass each record from the sheet to be procced.\n",
    "* Ckassify the text to process only the English tweets\n",
    "* If it is an English tweet pass it to the RegxTokenization. Here each sentence/tweet is processed to return a token list.\n",
    "* The tokens having length less than 3 are removed from the token list.\n",
    "* The tokens list is then used to filter out non the context independent stop words from the tokens list\n",
    "* This word list is then stored in dictionary and appended as lists in the clean data dictionary with the sheetname i.e  date as key.  \n",
    "* Additionally the token_list is stored as is in the token dictionary for further use to create bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Parsing the data and reading the data one sheet at a time\n",
    "for sheets in file.sheet_names:\n",
    "    df = pd.read_excel(file, sheets)\n",
    "    \n",
    "    #Handle the NA's in the sheet\n",
    "    df = df.dropna(axis=1, how='all')\n",
    "    df = df.dropna(axis=0, how='all')\n",
    "   \n",
    "    #Adjust the header for the columns in the df\n",
    "    for col in df.columns:\n",
    "        if 'Unnamed' in col:\n",
    "            df.columns = df.iloc[0]\n",
    "            \n",
    "    #Loop through the length of the dataframe in each sheet        \n",
    "    for i in range(1,len(df)):\n",
    "        #Checking for English language tweets\n",
    "        x = langid.classify(str(df.text.values[i]))\n",
    "        #Process the data only if it is in English langiage\n",
    "        if x[0] == 'en':\n",
    "            #Apply RegexTokenizer to make tokens from the sentence\n",
    "            tokenizer = RegexpTokenizer(r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\")\n",
    "            \n",
    "            #Apply tokenize to the tokenizer defined above\n",
    "            tokens = tokenizer.tokenize(str(df.text.values[i]))\n",
    "           \n",
    "            #Removing words less than 3 using regex\n",
    "            \n",
    "            #Regex expression to compile for the words having length less than 3\n",
    "            short_word =re.compile(r'^(..?)(?!.)')\n",
    "            \n",
    "            #Removing the tokens with length less than 3 from the tokens list\n",
    "            filtered = [ele.lower() for ele in tokens if not short_word.match(ele)]\n",
    "            \n",
    "            #Removing context independent stop words from the filtered token list\n",
    "            without_sw = [w for w in filtered if w not in stopwords_list]\n",
    "            \n",
    "            #Storing the without stopwords tokens list in the data dictionary\n",
    "            data_dict = without_sw\n",
    "           \n",
    "            #Storing the data dictionary to the data list\n",
    "            data_list.append(data_dict)  \n",
    "            \n",
    "            #Appending the tokens list to tokens list\n",
    "            token_list.append(tokens)\n",
    "    \n",
    "    #Storing the token list to token dictionary with sheetname as the key (which stores the date)\n",
    "    token_dict[sheets] = list(chain(*token_list))\n",
    "    \n",
    "    #Storing the without stopwords tokens list to clean data dictionary with sheetname as the key (which stores the date)\n",
    "    clean_dict[sheets] = list(chain(*data_list))\n",
    "    \n",
    "    #Re initializing the lists to store new values for the next data record\n",
    "    data_list = []\n",
    "    token_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Removing Rare Tokens and Context dependent words  (with the threshold set to more than 60 days) And Stemming\n",
    "\n",
    "* Rare tokens with the threshold set to 5 days is removed. \n",
    "* Context dependent words with threshold set to greater than 60 days is removed from the clean data dictionary. \n",
    "\n",
    "* Each unique word from the clean dictionary is combined to a list, which is passed to the FreqDist using the rare_tokens. \n",
    "* FreqDist returns the frequency for each of the value in the keys.\n",
    "* If the token is present for more than 60 days it is removed as context dependent frequent words. Other words which appear for less than 5 days are removed as rare tokens. \n",
    "\n",
    "* Removing rare tokens is very intuitive, as unique words as such as names, brands and html leftouts need to be removed for different NLP or machine learning tasks. An instnace of using names as a predictor for a text classification problem is a bad approach.\n",
    "\n",
    "* Removing context dependent words sometimes leads to less ambigiuty in solving a classification problem and might not serve the purpose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Creating the clean data tokens list for the clean dictionary\n",
    "clean_data_tokens = list(chain(*[set(value) for value in clean_dict.values()]))\n",
    "# Find the frequency distribution for the the lists\n",
    "rare_tokens = FreqDist(clean_data_tokens)\n",
    "\n",
    "#Deleting the values from the rare_tokens dictionary\n",
    "for k, v in list(rare_tokens.items()):\n",
    "    # Rare tokens to delete for tokens which have the frequency of less than 5\n",
    "    if v < 5:\n",
    "        del rare_tokens[k]\n",
    "    # Remove context independent words from the clean data for threshold set to greater than 60\n",
    "    if v > 60:\n",
    "        del rare_tokens[k]\n",
    "\n",
    "        \n",
    "#Stemming the data to eliminate words with same meanings \n",
    "clean_data = [stemmer.stem(word) for word in rare_tokens.keys()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Creating Unigrams \n",
    "\n",
    "* Here, using the already pre processed clean dictionary which has the tokens with less than length 3 and removed context independent stopwords.  \n",
    "> Even after we have filtered, for only for informative words, mostly there is a possibiity of multiple words representing the same maening in different forms and are mapped to the same word,but have a different spelling and structure because of the sentence context.  \n",
    "> For example, “connect”, “connecting”, “connected”, “connections”, and “connects” could all be used to illustrate the word connect.\n",
    "\n",
    "* Here the clean_dict is stemmed using Porter Stemmer.\n",
    "* This clean dictionary stemmed is used to find the FreqDist.\n",
    "* Of which only the most common 100 unigrams is stored in the dictionary for each key i.e date\n",
    "\n",
    "* Write the top 100 unigrams for each date in the output file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#Creating unigrams from the clean tokens dictionary \n",
    "#(Which is already tokenized and the stop words have been removed from it and tokens less than 3 have also been eliminated.)\n",
    "for k, v in clean_dict.items():\n",
    "    #Stemming each of the tokens\n",
    "    clean_dict_stem = [stemmer.stem(word) for word in v]\n",
    "    #Creating frequency Distribution and passing the dictionary with words\n",
    "    unigram = FreqDist(clean_dict_stem)\n",
    "    #Finding the most common 100 words in each key\n",
    "    unigram_dict[k] = unigram.most_common(100)\n",
    "\n",
    "#Creating Unigram.txt file\n",
    "fw = open('31224075_100uni.txt', 'w')\n",
    "for k, v in unigram_dict.items():\n",
    "    fw.write(k)\n",
    "    fw.write(':')\n",
    "    fw.write(str(v))\n",
    "    fw.write('\\n')\n",
    "fw.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Creating Bigrams \n",
    "\n",
    "* Using the token dictionary initially created with just the tokens\n",
    "* Creating N grams of length 2  by passing N =2 \n",
    "* Find the frequency Distribution for this bigram data\n",
    "* Sort the bigram dictfor each key i.e date\n",
    "\n",
    "* Write the bigrams_dict to bigrams output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Creating bigrams from the token dictionary\n",
    "for k, v in token_dict.items():\n",
    "    #Creating bigrams using Ngrams\n",
    "    bigrams = ngrams([x.lower() for x in v], n = 2)\n",
    "    #Finding the Frequency distribution\n",
    "    bigram = FreqDist(bigrams)\n",
    "    #Finding the 100 most common \n",
    "    bigram_dict[k] = bigram.most_common(100)\n",
    "\n",
    "#Sorting the key i.e date     \n",
    "sorted(bigram_dict.items(), key=lambda kv: kv[1], reverse=True)\n",
    "#print(token_dict)\n",
    "\n",
    "# Writing the Bigram.txt file\n",
    "fw = open('31224075_100bi.txt', 'w')\n",
    "for k, v in bigram_dict.items():\n",
    "    fw.write(k)\n",
    "    #print(k,v)\n",
    "    fw.write(':')\n",
    "    fw.write(str(v))\n",
    "    fw.write('\\n')\n",
    "#fw.write(str(bigram_dict))\n",
    "fw.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Vocab File\n",
    "\n",
    "* Using chain combine all the token lists from the clean dictionary to a list.\n",
    "* Using the Bigram_finder from BigramCollocationFinder in nltk and by using by_words bigrams are created from the token_list.\n",
    "* Using the nbest function and passing the bigram_measures as pmi set to 200 \n",
    "* Join the bigrams found as collocation words ('_') \n",
    "* Combine the collocation words found and the clean data to form a vocab list.\n",
    "* Sort the vocab to list the words in an ascending alphabetical format.\n",
    "* Storing the vocab words as key and index as value in the index dictionary.\n",
    "* Read the vocab list and write into the vocab output file in the required format. i.e (token:index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Creating the token list by combining the  clean_dict values\n",
    "token_list = list(chain(*clean_dict.values()))\n",
    "#Creating the bigrams from the token list\n",
    "finder = bigram_finder.from_words(token_list)\n",
    "#Finding the top 200 meaningful bigrams from the \n",
    "pre_collocation = finder.nbest(bigram_measures.pmi, 200)\n",
    "#Combining the bigrams to create collocations with '_'\n",
    "collocation = ['_'.join(value).lower() for value in pre_collocation]\n",
    "\n",
    "#Combine the clean data and the collocation words to form the vocab\n",
    "vocab = [*collocation, *set(clean_data)]\n",
    "\n",
    "#Sort the vocab in alphabetical order\n",
    "vocab.sort()\n",
    "\n",
    "#Creating Vocab.txt file\n",
    "fw = open('31224075_vocab.txt', 'w')\n",
    "for index, word in enumerate(vocab):\n",
    "    fw.write(word)\n",
    "    fw.write(':')\n",
    "    fw.write(str(index))\n",
    "    fw.write('\\n')\n",
    "    index_dict[word] = index\n",
    "fw.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create the Sparse matrix using Counter Vectorization\n",
    "\n",
    "* Use Count Vectorizer to convert a collection of text documents to a matrix of token counts.\n",
    "* Generating the count vector representation for each key i.e. date. \n",
    "* Re tokenize the callocation words with the clean dictionary tokens.\n",
    "* Stemming the callocated words \n",
    "* Use vector fit transform on the new tokenized words to get the data features. \n",
    "* Writing the processed sparse matrix in the output file format. And replacing the key word witht the index of that word.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Initializing the Counter Vector \n",
    "vectorizer = CountVectorizer(analyzer = \"word\") \n",
    "# Retokenize the callocation words\n",
    "mweToken = MWETokenizer(pre_collocation)\n",
    "\n",
    "#Write the counter Vector file\n",
    "fw = open('31224075_countVec.txt', 'w')\n",
    "for k, v in clean_dict.items():\n",
    "    word = (mweToken.tokenize(v))\n",
    "    token = []\n",
    "    for w in word:\n",
    "        if re.search('(_)', w) is None:\n",
    "            token.append(stemmer.stem(w))\n",
    "        else:\n",
    "            token.append(w)\n",
    "    data_features = vectorizer.fit_transform([' '.join(token)])\n",
    "    vocab_feature = vectorizer.get_feature_names()\n",
    "    fw.write(k)\n",
    "    for data, indices in zip(data_features.data, data_features.indices):\n",
    "        word = vocab_feature[indices]\n",
    "        index = index_dict.get(word, None)\n",
    "        #print(index)\n",
    "        if index == None:\n",
    "            continue\n",
    "        fw.write(','+str(index))\n",
    "        fw.write(':')\n",
    "        fw.write(str(data))\n",
    "    fw.write('\\n')\n",
    "    \n",
    "fw.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Some statistics Information from the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the most frequent words in the data after tokenization : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#Plotting most freq words\n",
    "plot_words = list(chain(*token_dict.values()))\n",
    "plot_words = FreqDist(plot_words)\n",
    "plot_words = list(reversed(plot_words.most_common(20)))\n",
    "print(plot_words)\n",
    "data_words = []\n",
    "words_counts = []\n",
    "for v in plot_words:\n",
    "    data_words.append(v[0])\n",
    "    words_counts.append(v[1])\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.plot(data_words, words_counts)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the most frequent token after removing the words less than length 3 and removing the stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#Plotting most freq words\n",
    "plot_words = list(chain(*clean_dict.values()))\n",
    "plot_words = FreqDist(plot_words)\n",
    "plot_words = list(reversed(plot_words.most_common(20)))\n",
    "print(plot_words)\n",
    "data_words = []\n",
    "words_counts = []\n",
    "for v in plot_words:\n",
    "    data_words.append(v[0])\n",
    "    words_counts.append(v[1])\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.plot(data_words, words_counts)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "words = list(chain(*clean_dict.values()))\n",
    "#vocab = set(words)\n",
    "lexical_diversity = len(words)/len(vocab)\n",
    "print (\"Vocabulary size: \",len(vocab))\n",
    "print (\"Total number of tokens: \", len(words))\n",
    "print (\"Lexical diversity: \", lexical_diversity)\n",
    "print (\"Total number of documents:\", len(token_dict))\n",
    "lens = [len(value) for value in token_dict.values()]\n",
    "print (\"Average document length:\", np.mean(lens))\n",
    "print (\"Maximun document length:\", np.max(lens))\n",
    "print (\"Minimun document length:\", np.min(lens))\n",
    "print (\"Standard deviation of document length:\", np.std(lens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion & Learnings:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost all of the NLP or Machine learning tasks, require data to be preprocessed before training a model or to perform further tasks.\n",
    "Most of the tasks and models cannot use raw text directly, so it requires text to be preprocessed. The preprocessing methods can be different depending on the nature of the tasks. \n",
    "Why use NLTK to perform most of the tasks?\n",
    "* NLTK is one among the many leading platforms in dealing with language data.\n",
    "* Provides easy-to-use APIs for many text preprocessing methods. \n",
    "\n",
    "Some of the important tasks in data preprocessing include:\n",
    "* Lowercase\n",
    "* Tokenization\n",
    "* Stopword Filtering\n",
    "* Stemming\n",
    "* Finding additional colocated words from ngrams\n",
    "* POS Tagger\n",
    "\n",
    "Each of which is important to determine the feature data for the model. The different unigrams, bigrams and n-grams from the data are are used for a variety of things. Some use cases include auto completion of sentences, auto spell check and we can also look for the grammar in the sentence. \n",
    "\n",
    "As most of the machine learning features only work on numbers, there is a need to convert this character/text data to numerical values. Hence the use of sparse matrix using count vectorization. In order to analyze the text it needs to be converted to numbers.\n",
    "A few examples of using this approach could result in Text Summarization, Sentimental analysis of data. Topic Modelling etc. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. http://www.nltk.org/api/nltk.tokenize.html\n",
    "2. http://www.nltk.org/book/ch05.html\n",
    "3. https://www.nltk.org/howto/collocations.html\n",
    "4. https://medium.com/python-in-plain-english/collocation-discovery-with-pmi-3bde8f351833"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
